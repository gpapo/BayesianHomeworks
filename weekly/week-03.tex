\chapter{Non informative priors}

\section{Exercise 3.10}

\subsection{Data}
\begin{itemize}
 \item $ \psi = g(\theta) $ where $ g $ is a monotone function
 \item $ h(\cdot) = g^{-1}(\cdot) $, that is $ \theta = h(\psi) $
 \item $ p_\theta(\theta) = \text{PDF of } \theta \implies
       p_\psi(\psi) = p_\theta(h(\psi)) \cdot \left|\frac{dh}{d\psi}\right|$
\end{itemize}

\subsection{Questions}
\begin{enumerate}
 \item Let $ \theta \sim Beta(a, b) $ and $ \psi = \logit(\theta) $. Obtain $ p_\psi $ and
       plot it for the case $ a = b = 1 $.
 \item Let $ \theta \sim Gamma(a, b) $ and $ \psi = \log(\theta) $. Obtain $ p_\psi $ and
       plot it for the case $ a = b = 1 $.
\end{enumerate}

\subsection{Solutions}
\begin{enumerate}
 \item The inverse function of $ \logit(\cdot) $ is known to be
       $ h(\psi) = \frac{\exp(\psi)}{1 + \exp(x\psi)} $, and the derivative w.r.t. $\psi$ of
       $ h(\psi) $ is
       \begin{align*}
        \frac{\partial h(\psi)}{\psi} & = \frac{\exp(\psi)(1 + \exp(\psi) - \exp(2\psi)}{(1 + \exp(\psi))^2} \\
                                      & = \frac{\exp(\psi)}{(1 + \exp(\psi))^2}
       \end{align*}
       So we can write the PDF of $\psi$ as
       \begin{align*}
        p_\psi(\psi) & = \frac{1}{B(a, b)} \left(\frac{\exp(\psi)}{1 + \exp(\psi)}\right)^{a - 1}
        \left(1 - \frac{\exp(\psi)}{1 + \exp(\psi)}\right)^{b - 1}
        \frac{\exp(\psi)}{(1 + \exp(\psi))^2} \\
                     & = \frac{1}{B(a, b)} \frac{\exp(\psi)^{a - 1}}{(1 - \exp(\psi))^{a - 1}}
        \frac{1}{(1 + \exp(\psi))^{b - 1}}
        \frac{\exp(\psi)}{(1 + \exp(\psi))^2} \\
                     & = \frac{1}{B(a, b)} \frac{\exp(\psi)^{a}}{(1 + \exp(\psi))^{a + b}}
       \end{align*}
       And in the case that $ a = b = 1 $ the plot is: \\
       \includegraphics[width=\textwidth]{julia-scripts/week-03_plot1.png}
       % Note: it seems very similar to a Normal distribution.
 \item The inverse function of $ \log(\theta) $ is known to be $ h(\psi) = \exp(\psi) $,
       and the derivative w.r.t. $\psi$ of $ h(\psi) $ is
       \[ \frac{\partial h(\psi)}{\partial \psi} = \exp(\psi) \]
       So we can write the PDF of $ \psi $ as
       \begin{align*}
        p_\psi(\psi) & = \frac{b ^ a}{\Gamma(a)} \exp(\psi)^{a - 1} \exp(-b\exp(\psi)) \exp(\psi) \\
                     & = \frac{b ^ a}{\Gamma(a)} \exp(a\psi - b\exp(\psi))
       \end{align*}
       And in the case that $ a = b = 1 $ the plot is: \\
       \includegraphics[width=\textwidth]{julia-scripts/week-03_plot2.png}
\end{enumerate}

\section{Exercise 3.14}

\subsection{Data}

\subsection{Questions}
\begin{enumerate}
 \item Given $ Y_1 \ldots Y_n \sim \text{Bernoulli}(\theta) $ find the MLE of $\theta$ and
       $ \frac{J(\theta)}{n} $.
 \item Find a PDF $ p_U(\theta) $ such that $ \log p_U(\theta) = l(\theta | \mathbf{y}) / n + c $
       where $ c $ is a constant that does not depend on $\theta$. Compute $ -\partial^2 \log p_U(\theta) / \partial^2 \theta $.
 \item Obtain a PDF for $\theta$ that is proportional to $ p_U(\theta) p(\mathbf{y} | \theta) $.
       Can this be considered a posterior for $ \theta $?
 \item Repeat previous points with $ Y_1 \ldots Y_n \sim \text{Poisson}(\theta) $.
\end{enumerate}

\subsection{Solutions}
\begin{enumerate}
 \item \begin{align*}
       \hat{\theta}_{MLE} & = \argmin_\theta l(\theta | \mathbf{y}) \\
       & = \argmin_\theta \log \left( \prod_i \theta ^ {y_i} (1 - \theta)^{1 - y_i} \right) \\
       & = \argmin_\theta \sum_i \log \left( \theta ^ {y_i} (1 - \theta) ^ {1 - y_i} \right) \\
       & = \argmin_\theta \log(\theta) \sum_i y_i + \log(1 - \theta) \sum_i (1 - y_i) \\
       & = \argmin_\theta \log(\theta) \sum_i y_i + n \log(1 - \theta) - \log(1 - \theta) \sum_i (y_i)
 \end{align*}
 To find the minimum we compute the zeros of $ \partial l(\theta | \mathbf{y}) / \partial \theta $
 \begin{align*}
  0 & = \frac{\partial l(\theta | \mathbf{y})}{\partial \theta}                                   \\
    & = \frac{\sum_i y_i}{\theta} - \frac{n}{1 - \theta} + \frac{\sum_i y_i}{1 - \theta}          \\
    & = \frac{\sum_i y_i - \theta \sum_i y_i - \theta n + \theta \sum_i y_i}{\theta (1 - \theta)} \\
    & = \frac{\sum_i y_i - \theta n}{\theta (1 - \theta)}
 \end{align*}
 Thus, if $ \theta \notin \left\{0, 1\right\} $, $ \hat \theta_{MLE} = \frac{\sum_i y_i}{n} $.
 \begin{align*}
  - \frac{\partial^2 l(\theta | \mathbf{y})}{n \partial^2 \theta}
    & = - \frac{-n \theta (1 - \theta) - (\sum_i y_i - n \theta)(1 - 2\theta)}{n \theta^2 (1 - \theta)^2} \\
    & = - \frac{n \theta^2 - n \theta - \sum_i y_i + 2\theta \sum_i y_i + n \theta - 2 n \theta^2}
  {n \theta^2 (1 - \theta^2)} \\
    & = \frac{n \theta ^ 2 - 2 \theta \sum_i y_i + \sum_i y_i}{n \theta^2 (1 - \theta)^2}                 \\
    & = \frac{\theta ^ 2 - 2 \theta \hat \theta_{MLE} + \hat \theta_{MLE}}{\theta^2 (1 - \theta)^2}
 \end{align*}
 \item The constrains on $ p_U(\theta) $ imply that
       \begin{align*}
        p_U(\theta) & = c \sqrt[n]{\prod_i \theta ^ {y_i} (1 - \theta) ^ {1 - y_i}}          \\
                    & = c \prod_i \theta ^ {y_i / n} (1 - \theta) ^ {(1 - y_i) / n}          \\
                    & = c \; \theta ^ {\sum_i y_i / n} (1 - \theta) ^ {\sum_i (1 - y_i) / n} \\
                    & = c \; \theta ^ {\sum_i y_i / n} (1 - \theta) ^ {1 - \sum_i y_i / n}
       \end{align*}
       where $ c $ is the normalization constant.
       \begin{align*}
        - \frac{\partial^2 \log p_U(\theta)}{\partial^2 \theta}
          & = - \frac{\partial^2 l(\theta | \mathbf{y}) / n + c}{\partial^2 \theta}                       \\
          & = - \frac{\partial^2 l(\theta | \mathbf{y})}{\partial^2 \theta} / n                           \\
          & = \frac{\theta ^ 2 - 2 \theta \hat \theta_{MLE} + \hat \theta_{MLE}}{\theta^2 (1 - \theta)^2}
       \end{align*}
 \item Such a PDF would have the form
       \begin{align*}
        p(\theta | \mathbf{y}) & = c \cdot p_U(\theta) p(\mathbf{y} | \theta)                                                       \\
                               & = c \; \theta ^ {\sum_i y_i / n} ( 1 - \theta)^{1 - \sum_i y_i / n} \cdot
        \theta ^ {\sum_i y_i} (1 - \theta) ^ {\sum_i (1 - y_i)} \\
                               & = c \; \theta ^ {\sum_i y_i (1 + \frac{1}{n})} (1 - \theta) ^ {\sum_i (1 - y_i) (1 + \frac{1}{n})} \\
                               & = c \; \theta ^ {\sum_i y_i (1 + \frac{1}{n})} (1 - \theta) ^ {(n - \sum_i y_i) (1 + \frac{1}{n})}
       \end{align*}
       Where $ c = \int_{S_\theta} p(\theta, \mathbf{y}) \partial \theta $ to guarantee that
       the PDF is proper. We can observe that the obtained PDF is a Beta distribution with
       parameters $ \sum_i y_i (1 + \frac{1}{n}) + 1 $ and $ (n - \sum_i y_i)(1 + \frac{1}{n}) + 1 $.
       It is a posterior because it is the product of a prior and a conditioned probability. Moreover,
       it is a case of conjugate prior.
 \item Same steps: simplify the log-likelihood, find the first derivative and constrain to zero.
       \begin{align*}
        \hat \theta_{MLE} & = \argmin_\theta l_\text{Poisson}(\theta | \mathbf{y})                               \\
                          & = \argmin_\theta \log \left( \prod_i \frac{\exp(-\theta) \theta^{y_i}}{y_i!} \right) \\
                          & = \argmin_\theta \sum_i \log \frac{\exp(-\theta) \theta^{y_i}}{y_i !}                \\
                          & = \argmin_\theta -n \theta + \log \theta \sum y_i - \sum \log y_i !
       \end{align*}
       \begin{align*}
        0 & = \frac{\partial l(\theta | \mathbf{y})}{\partial \theta} \\
          & = -n + \frac{\sum_i y_i}{\theta}
       \end{align*}
       Thus $ \hat \theta_{MLE} = \sum_i y_i / n $.
       \begin{align*}
        -\frac{\partial^2 l(\theta | \mathbf{y})}{n \partial^2 \theta}
          & = \frac{\sum_i y_i}{n \theta^2}      \\
          & = \frac{\hat \theta_{MLE}}{\theta^2}
       \end{align*}
       In this case the PDF $ p_U $ would be
       \begin{align*}
        p_U(\theta) & \propto \sqrt[n]{\prod_i \exp(-\theta) \theta ^ {y_i}} \\
                    & = \exp(-\theta) \theta ^ {\sum_i y_i / n}
       \end{align*}
       Thus \begin{align*}
       - \frac{\partial^2 \log p_U(\theta)}{\partial^2 \theta} & = \frac{\sum_i y_i}{n \theta^2}
 \end{align*}
 While the posterior is
 \begin{align*}
  p(\theta | \mathbf{y}) & \propto \exp(-\theta) \theta ^ {\sum_i y_i / n} \prod_i \exp(-\theta) \theta ^ {y_i} \\
                         & = \exp(- (n + 1) \theta) \theta ^ {\sum_i y_i (1 + \frac{1}{n})}
 \end{align*}
 This time the posterior is a Gamma distribution with parameters $ \sum_i y_i (1 + \frac{1}{n}) $ and $ n + 1 $.
\end{enumerate}

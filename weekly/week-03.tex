\chapter{Non informative priors}

\section{Exercise 3.10, Hoff}

\subsection{Data}
\begin{itemize}
  \item $ \psi = g(\theta) $ where $ g $ is a monotone function
  \item $ h(\parm) = g^{-1}(\parm) $, that is $ \theta = h(\psi) $
  \item $ p_\theta(\theta) = \text{PDF of } \theta \implies
          p_\psi(\psi) = p_\theta(h(\psi)) \cdot \left|\frac{dh}{d\psi}\right|$
\end{itemize}

\subsection{Questions}
\begin{enumerate}
  \item Let $ \theta \sim Beta(a, b) $ and $ \psi = \logit(\theta) $. Obtain $ p_\psi $ and
        plot it for the case $ a = b = 1 $.
  \item Let $ \theta \sim Gamma(a, b) $ and $ \psi = \log(\theta) $. Obtain $ p_\psi $ and
        plot it for the case $ a = b = 1 $.
\end{enumerate}

\subsection{Solutions}
\begin{enumerate}
  \item The inverse function of $ \logit(\cdot) $ is known to be
        $ h(\psi) = \frac{\exp(\psi)}{1 + \exp(x\psi)} $, and the derivative w.r.t. $\psi$ of
        $ h(\psi) $ is
        \begin{align*}
          \frac{\partial h(\psi)}{\psi} & = \frac{\exp(\psi)(1 + \exp(\psi) - \exp(2\psi)}{(1 + \exp(\psi))^2} \\
                                        & = \frac{\exp(\psi)}{(1 + \exp(\psi))^2}
        \end{align*}
        So we can write the PDF of $\psi$ as
        \begin{align*}
          p_\psi(\psi)
           & = \frac{1}{\Beta(a, b)} \left(\frac{\exp(\psi)}{1 + \exp(\psi)}\right)^{a - 1}
          \left(1 - \frac{\exp(\psi)}{1 + \exp(\psi)}\right)^{b - 1}
          \frac{\exp(\psi)}{(1 + \exp(\psi))^2}                                         \\
           & = \frac{1}{\Beta(a, b)} \frac{\exp(\psi)^{a - 1}}{(1 - \exp(\psi))^{a - 1}}
          \frac{1}{(1 + \exp(\psi))^{b - 1}}
          \frac{\exp(\psi)}{(1 + \exp(\psi))^2}                                         \\
           & = \frac{1}{\Beta(a, b)} \frac{\exp(\psi)^{a}}{(1 + \exp(\psi))^{a + b}}
        \end{align*}
        \begin{figure}[H]
          \centering
          \includegraphics[width=0.8\linewidth]{julia-scripts/week-03_plot1.png}
          \caption{Case for $ a = b = 1 $.}
        \end{figure}
        % Note: it seems very similar to a Normal distribution.
  \item The inverse function of $ \log(\theta) $ is known to be $ h(\psi) = \exp(\psi) $,
        and the derivative w.r.t. $\psi$ of $ h(\psi) $ is
        \[ \frac{\partial h(\psi)}{\partial \psi} = \exp(\psi) \]
        So we can write the PDF of $ \psi $ as
        \begin{align*}
          p_\psi(\psi) & = \frac{b ^ a}{\Gamma(a)} \exp(\psi)^{a - 1} \exp(-b\exp(\psi)) \exp(\psi) \\
                       & = \frac{b ^ a}{\Gamma(a)} \exp(a\psi - b\exp(\psi))
        \end{align*}
        \begin{figure}[H]
          \centering
          \includegraphics[width=0.8\linewidth]{julia-scripts/week-03_plot2.png}
          \caption{Case for $ a = b = 1 $.}
        \end{figure}
\end{enumerate}

\section{Exercise 3.14, Hoff}

\subsection{Data}

\subsection{Questions}
\begin{enumerate}
  \item Given $ Y_1 \ldots Y_n \sim \Bernoulli(\theta) $ find the MLE of $\theta$ and
        $ \frac{J(\theta)}{n} $.
  \item Find a PDF $ p_U(\theta) $ such that $ \log p_U(\theta) = \loglik(\theta | \vect{y}) / n + c $
        where $ c $ is a constant that does not depend on $\theta$. Compute $ -\partial^2 \log p_U(\theta) / \partial^2 \theta $.
  \item Obtain a PDF for $\theta$ that is proportional to $ p_U(\theta) p(\vect{y} | \theta) $.
        Can this be considered a posterior for $ \theta $?
  \item Repeat previous points with $ Y_1 \ldots Y_n \sim \Poisson(\theta) $.
\end{enumerate}

\subsection{Solutions}
\begin{enumerate}
  \item \begin{align*}
          \hat{\theta}_{MLE} & = \argmin_\theta \loglik(\theta | \vect{y})                                                       \\
                             & = \argmin_\theta \log \left\{ \prod_i \theta ^ {y_i} (1 - \theta)^{1 - y_i} \right\}        \\
                             & = \argmin_\theta \sum_i \log \left( \theta ^ {y_i} (1 - \theta) ^ {1 - y_i} \right)         \\
                             & = \argmin_\theta \log(\theta) \sum_i y_i + \log(1 - \theta) \sum_i (1 - y_i)                \\
                             & = \argmin_\theta \log(\theta) \sum_i y_i + n \log(1 - \theta) - \log(1 - \theta) \sum_i y_i
        \end{align*}
        To find the minimum we compute the zeros of $ \partial l(\theta | \vect{y}) / \partial \theta $
        \begin{align*}
          0 & = \frac{\partial \loglik(\theta | \vect{y})}{\partial \theta}                                     \\
            & = \frac{\sum_i y_i}{\theta} - \frac{n}{1 - \theta} + \frac{\sum_i y_i}{1 - \theta}          \\
            & = \frac{\sum_i y_i - \theta \sum_i y_i - \theta n + \theta \sum_i y_i}{\theta (1 - \theta)} \\
            & = \frac{\sum_i y_i - \theta n}{\theta (1 - \theta)}
        \end{align*}
        Thus, if $ \theta \notin \left\{0, 1\right\} $, $ \hat \theta_{MLE} = \frac{\sum_i y_i}{n} $.
        \begin{align*}
          - \frac{\partial^2 \loglik(\theta | \vect{y})}{n \partial^2 \theta}
           & = - \frac{-n \theta (1 - \theta) - (\sum_i y_i - n \theta)(1 - 2\theta)}{n \theta^2 (1 - \theta)^2} \\
           & = - \frac{n \theta^2 - n \theta - \sum_i y_i + 2\theta \sum_i y_i + n \theta - 2 n \theta^2}
          {n \theta^2 (1 - \theta^2)}                                                                            \\
           & = \frac{n \theta ^ 2 - 2 \theta \sum_i y_i + \sum_i y_i}{n \theta^2 (1 - \theta)^2}                 \\
           & = \frac{\theta ^ 2 - 2 \theta \hat \theta_{MLE} + \hat \theta_{MLE}}{\theta^2 (1 - \theta)^2}
        \end{align*}
  \item The constrains on $ p_U(\theta) $ imply that
        \begin{align*}
          p_U(\theta) & = c \prod_i \theta ^ {y_i / n} (1 - \theta) ^ {(1 - y_i) / n}          \\
                      & = c \; \theta ^ {\sum_i y_i / n} (1 - \theta) ^ {\sum_i (1 - y_i) / n} \\
                      & = c \; \theta ^ {\sum_i y_i / n} (1 - \theta) ^ {1 - \sum_i y_i / n}
        \end{align*}
        where $ c $ is the normalization constant.
        \begin{align*}
          - \frac{\partial^2 \log p_U(\theta)}{\partial^2 \theta}
           & = - \frac{\partial^2 \loglik(\theta | \vect{y}) / n + c}{\partial^2 \theta}                         \\
           & = - \frac{\partial^2 \loglik(\theta | \vect{y})}{\partial^2 \theta} / n                             \\
           & = \frac{\theta ^ 2 - 2 \theta \hat \theta_{MLE} + \hat \theta_{MLE}}{\theta^2 (1 - \theta)^2}
        \end{align*}
  \item Such a PDF would have the form
        \begin{align*}
          p(\theta | \vect{y}) & = c \cdot p_U(\theta) p(\vect{y} | \theta)                                                         \\
                               & = c \; \theta ^ {\sum_i y_i / n} ( 1 - \theta)^{1 - \sum_i y_i / n} \cdot
          \theta ^ {\sum_i y_i} (1 - \theta) ^ {\sum_i (1 - y_i)}                                                                   \\
                               & = c \; \theta ^ {\sum_i y_i (1 + \frac{1}{n})} (1 - \theta) ^ {\sum_i (1 - y_i) (1 + \frac{1}{n})} \\
                               & = c \; \theta ^ {\sum_i y_i (1 + \frac{1}{n})} (1 - \theta) ^ {(n - \sum_i y_i) (1 + \frac{1}{n})}
        \end{align*}
        Where $ c = \int_{S_\theta} p(\theta, \vect{y}) \partial \theta $ to guarantee that
        the PDF is proper. We can observe that the obtained PDF is a Beta distribution with
        parameters $ \sum_i y_i (1 + \frac{1}{n}) + 1 $ and $ (n - \sum_i y_i)(1 + \frac{1}{n}) + 1 $.
        It is a posterior because it is the product of a prior and a conditioned probability. Moreover,
        it is a case of conjugate prior.
  \item Same steps: simplify the log-likelihood, find the first derivative and constrain to zero.
        \begin{align*}
          \hat \theta_{MLE} & = \argmin_\theta \loglik_\text{Poisson}(\theta | \vect{y})                                 \\
                            & = \argmin_\theta \log \left( \prod_i \frac{\exp(-\theta) \theta^{y_i}}{y_i!} \right) \\
                            & = \argmin_\theta \sum_i \log \frac{\exp(-\theta) \theta^{y_i}}{y_i !}                \\
                            & = \argmin_\theta -n \theta + \log \theta \sum y_i - \sum \log y_i !
        \end{align*}
        \begin{align*}
          0 & = \frac{\partial \loglik(\theta | \vect{y})}{\partial \theta} \\
            & = -n + \frac{\sum_i y_i}{\theta}
        \end{align*}
        Thus $ \hat \theta_{MLE} = \sum_i y_i / n $.
        \begin{align*}
          -\frac{\partial^2 \loglik(\theta | \vect{y})}{n \partial^2 \theta}
           & = \frac{\sum_i y_i}{n \theta^2}      \\
           & = \frac{\hat \theta_{MLE}}{\theta^2}
        \end{align*}
        In this case the PDF $ p_U $ would be
        \begin{align*}
          p_U(\theta) & \propto \sqrt[n]{\prod_i \exp(-\theta) \theta ^ {y_i}} \\
                      & = \exp(-\theta) \theta ^ {\sum_i y_i / n}
        \end{align*}
        Thus \begin{align*}
          - \frac{\partial^2 \log p_U(\theta)}{\partial^2 \theta} & = \frac{\sum_i y_i}{n \theta^2}
        \end{align*}
        While the posterior is
        \begin{align*}
          p(\theta | \vect{y}) & \propto \exp(-\theta) \theta ^ {\sum_i y_i / n} \prod_i \exp(-\theta) \theta ^ {y_i} \\
                               & = \exp(- (n + 1) \theta) \theta ^ {\sum_i y_i (1 + \frac{1}{n})}
        \end{align*}
        This time the posterior is a Gamma distribution with parameters $ \sum_i y_i (1 + \frac{1}{n}) $ and $ n + 1 $.
\end{enumerate}
